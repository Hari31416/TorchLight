# Mechanistic Interpretability

Some code and notes on mechanistic interpretability of machine learning models. This, for now, has a focus on computer vision. The module `feat_viz` contains code for visualizing what a layer, channel or neuron learns. A large part of the module is either taken or insipired by the [lucent](https://github.com/greentfrapp/lucent/tree/dev) library (which, in turn, is [lucid](https://github.com/tensorflow/lucid), adapted for PyTorch.)
